## 🗓️날짜 및 시간
2025년 3월 23일 일요일 20:00-22:00

## 🗽장소
온라인 zoom

## 🙇🏻‍♂️참여자
송샘, 이서진, 백승욱, 박보성

## 🏆목표
기술 구현 조사 및 캡스톤 평가표 작성

---
## 캡스톤 평가표 1차 작성
![Image](https://github.com/user-attachments/assets/714a416a-1b1f-46b3-b746-26a9c3a96dfa)

---
## <이서진>
## 3D point cloud를 이용한 발 측정
### 필요한 기능 (해야 하는 것들)
1. 발 3D Point Cloud 
    1. **하드웨어 구축**
        1. 하드웨어(ex. depth 카메라) 연결
        2. 사용자가 발 측정 시 필요한 하드웨어 구축(ex. 모터 사용을 통한 카메라 회전 등)
            
            → 이 부분을 어떻게 구현할건지 정하고 필요한 센서 찾아봐야할듯
            
    2. **여러 각도에서 촬영한 센서 데이터 획득**
    3. **여러 각도에서 촬영한 센서 데이터를 3D point cloud로 변환**
    4. **여러 각도에서 촬영한 센서 데이터에서 필요 없는 배경 제거**
    5. **여러 각도에서 얻은 Point Cloud를 하나의 3D 모델로 정합**
    6. 완성된 Point Cloud 시각화
2. **발 데이터 분석하는 코드 구현 → 물체의 실제 크기 측정**
    1. **발 길이, 발 볼 너비 측정**
        1. 촬영된 발 형상 3D 점군 데이터를 Z 축 기준으로 하여 10mm 이하 영역만을 픽셀 당 1mm를 갖는 2차원 평면으로 변환한다.
        2. 변환된 점군 데이터는 Canny Edge를 이용하여 윤곽선 영역을 검출한다.
        3. 윤곽선 영역은 Min Area Rect로 발을 감싸는 최소 사각형 영역을 검출하여 측정의 기준으로 사용한다. 
        4. 세로 길이는 발 길이, 가로 길이는 발볼 너비가 된다. 등
    2. **발등 높이 측정**
    3. **아치 유형 측정**
3. **GUI 설계 및 개발**
    1. 사용자가 A 버튼을 누르면 측정 시작 등 
    2. 실시간 스트리밍된 데이터를 3D View로 표시
    3. 측정 결과 표시 및 파일 저장 기능 추가 등

### 어떤 방법을 통해 해당 기술을 구현할 수 있는지 조사해야 할듯
- depth camera 데이터 수집 (하드웨어 연결 등)
- 수집한 데이터 point cloud로 변환
- 수집한 데이터에서 필요없는 배경 제거
    - 
- 여러 각도에서 수집한 point cloud 하나의 3D 모델로 정합
- point cloud 시각화

---
### 역할 분담
- 하드웨어 구축 & 센서 데이터 획득
    - Depth Camera에서 실시간 데이터 가져오기
    - 사용자가 발 측정 시 필요한 하드웨어 구축(ex. 모터 사용을 통한 카메라 회전 등)
- 실시간 데이터 스트리밍 및 전처리
    - 각 Depth Camera가 촬영한 depth 데이터를 3D point cloud로 변환
    - 각 Depth Camera가 촬영한 depth 데이터에서 필요 없는 배경 제거
        → 한 대의 카메라 기준으로 각각 독립적인 Point Cloud 생성
- 3D 모델 생성 및 데이터 분석
    - 각각의 카메라에서 얻은 Point Cloud를 하나의 3D 모델로 합치는 과정
        (카메라 별로 좌표계가 다르기 때문에 정렬과 정합이 필요)
        (여러 개의 다른 뷰에서 촬영된 데이터를 하나의 발 3D 모델로 결합)
    - 측정된 발 데이터를 분석하는 코드 초기 버전 구현
- GUI 개발 및 사용자 인터페이스
    - GUI 설계 및 개발
    - 실시간 스트리밍된 데이터를 3D View로 표시
    - 측정 결과 표시 및 파일 저장 기능 추가 등

---
### 필요한 재료
depth camera : Intel Realsense SR305 → 13만원 정도
https://prod.danawa.com/info/?pcode=16049105

### 참고 자료
[1. Design of foot parameter measurement system in 3D Point cloud.pdf](attachment:7baebfcb-bf1f-41ac-a4ea-b915ec6ea3cd:1._Design_of_foot_parameter_measurement_system_in_3D_Point_cloud.pdf)
[2. Design of a foot shape extraction system for foot parameter measurement.pdf](attachment:5ecdb02b-0ee4-41e9-b11c-9c7aa4b2c769:2._Design_of_a_foot_shape_extraction_system_for_foot_parameter_measurement.pdf)
https://www.youtube.com/watch?v=kiy8Bd6DAwM

---
## <송샘>
### **Depth Camera를 활용한 발바닥 3D 스캔 정리**
### 발바닥만 측정해도 되는 이유:
1. **발 모양의 주요 기준**은 발바닥의 **아치 높이**와 **발 넓이**:
    - 발바닥의 형태(특히 아치의 높이)는 **발의 안정성**과 **착용감을 좌우**하는 중요한 요소야.
    - 발바닥의 **넓이**는 **러닝화의 너비**와도 밀접하게 관련이 있어.
    - **발바닥**만 측정하면 **이 두 가지 주요 요소**를 정확히 파악할 수 있어.
2. **발등은 대부분의 경우 불필요**:
    - 발등은 **발의 높이**를 측정하는 부분인데, 러닝화 추천에서 가장 중요한 건 **발바닥의 형태**와 **발의 너비**야.
    - 발등의 높이는 일부 신발 모델에서 착용감을 맞추는 데 중요할 수 있지만, 대부분의 러닝화는 **발바닥**의 특성(아치 높이, 발 넓이)에 따라 추천이 가능해.
3. **러닝화의 착용감**은 발바닥의 **압력 분포와 아치 형성**에 의존:
    - 러닝화는 **발의 아치**에 맞춰 **지지력을 제공**하고, 발바닥의 **압력 분포**를 고려하여 **충격 흡수**가 필요해.
    - 발바닥만 측정해도 **아치의 높이**와 **넓이**를 기반으로 **러닝화의 최적화된 피팅**을 할 수 있어.
4. 발바닥으로도 아치 측정이 가능함
    - 발바닥만 측정해도 **아치 측정**이 가능하고, 발의 **아치 높이**와 **형태**에 맞춰 **러닝화 추천**을 할 수 있어. 발등까지 스캔할 필요는 없고, 발바닥만으로도 충분히 효과적인 추천을 할 수 있기 때문에, **발등까지 측정할 필요는 없을 수 있어**.

---
### **1. 개요**
- **목표:** Depth Camera를 이용해 발바닥을 3D로 스캔하고, Point Cloud로 변환하여 `.ply` 파일로 저장
- **사용 장비:** Intel RealSense Depth Camera
- **개발 환경:** Visual Studio, C++, imgui, RealSense SDK
- **LiDAR 센서가 필요한 경우**
    1. **고정밀 3D 스캔**:
        - **정밀한 발바닥 3D 스캔**을 원한다면 **LiDAR**가 매우 유용.
        - **Depth 카메라**는 일반적으로 **정확도가 떨어지거나** **어두운 환경에서 성능이 저하**될 수 있음.
        - LiDAR는 **빛을 이용한 거리 측정**으로 **어두운 환경에서도 우수한 성능**을 발휘.
    2. **복잡한 표면 측정**:
        - 발바닥처럼 **불규칙한 표면**이나 **세밀한 표정**을 정확하게 스캔하려면 LiDAR가 더 나은 선택이 될 수 있어.
        - LiDAR는 **정밀한 세밀한 표면 정보**를 측정할 수 있어, 특히 **고해상도 포인트 클라우드**를 얻는 데 유리함.
    3. **대상 물체의 빠른 스캔**:
        - **실시간 스캔**을 요구하는 경우, LiDAR는 매우 **빠르게 3D 맵**을 생성할 수 있기 때문에 유리할 수 있음.
---
### **2. 개발 과정**
### **(1) Depth 카메라를 활용한 촬영 및 후처리**
1. **Depth 카메라 촬영**
    - RealSense Camera로 발바닥을 촬영하여 Depth 및 컬러 이미지 획득
2. **Depth 이미지와 컬러 이미지 분할**
    - Depth 정보와 컬러 정보를 각각 분리하여 처리
3. **Depth 추출 및 색상 표현**
    - Depth 정보를 기반으로 색상으로 변환하여 시각적으로 출력
4. **원하는 이미지만 나오도록 후처리**
    - 노이즈 제거, 특정 거리 이상의 Depth 정보 필터링 (`Max Distance = 0.300m`)
5. **결과 화면 송출**
    - Viewport에서 Depth Color 이미지 출력
6. **Point Cloud 변환 및 `.ply` 파일 저장**
    - Depth 정보를 기반으로 포인트 클라우드(Point Cloud) 데이터 생성 및 `.ply` 파일로 저장

---
### **3. 주요 코드 설명**
```cpp
thr_filter.set_option(RS2_OPTION_MAX_DISTANCE, 0.300)
```
- Depth 정보 중 0.3m 이상 떨어진 부분을 제외하여 불필요한 정보 제거
```cpp
if(capture)
{
	rs2::frame f;

	// 1. Filtered Depth 및 Points 데이터 받아오기
	if(filtered_data.poll_for_frame(&f))
	{
		// 2. Depth 데이터를 색상화 (Color Mapping)
		colored_depth = color_map.process(f);

		// 3. Depth Color를 Point Cloud에 매핑
		filtered_pc.map_to(colored_depth);

		// 4. Depth 데이터를 Point Cloud로 변환
		filtered_points = filtered_pc.calculate(f);

		// 5. .ply 파일로 저장
		filtered_points.export_to_ply("test.ply", colored_depth);
		capture=false;
	}
}

```
- `poll_for_frame(&f)`: 새로운 Depth 데이터를 가져옴
- `color_map.process(f)`: Depth 데이터를 색상으로 변환
- `filtered_pc.map_to(colored_depth)`: 색상 정보를 Point Cloud 데이터에 매핑
- `filtered_pc.calculate(f)`: Depth 데이터를 Point Cloud로 변환
- `export_to_ply("test.ply", colored_depth)`: 최종 Point Cloud 데이터를 `.ply` 파일로 저장

**1️⃣ 거리 제한 필터 적용 (MAX_DISTANCE = 0.300)**
```cpp
thr_filter.set_option(RS2_OPTION_MAX_DISTANCE, 0.300);
```

**🔍 코드 설명**
- `thr_filter.set_option(...)` → **RealSense SDK의 필터 옵션을 설정하는 함수**.
- `RS2_OPTION_MAX_DISTANCE` → **최대 거리 필터 옵션**, 0.3m(30cm)보다 먼 거리의 데이터는 제거됨.
- **👀 왜 필요해?**
    - Depth 카메라는 기본적으로 **모든 거리의 데이터를 받아옴**.
    - 하지만 **발바닥 스캔만 필요**하기 때문에, **30cm보다 먼 데이터는 제거**하여 **배경 정보나 불필요한 노이즈를 줄일 수 있음**.
    - 예를 들어, 카메라 뒤쪽에 있는 **벽이나 바닥이 Depth 데이터에 포함되지 않도록 필터링**.

---

**2️⃣ .ply 파일 저장 로직 (3D 포인트 클라우드 저장)**
```cpp
if(capture)
{
    rs2::frame f;

    // 1. 필터링된 Depth 데이터를 받아옴
    if(filtered_data.poll_for_frame(&f))
    {
        // 2. Depth 데이터를 색상으로 변환 (시각화)
        colored_depth = color_map.process(f);

        // 3. 변환된 Depth 데이터를 포인트 클라우드에 매핑
        filtered_pc.map_to(colored_depth);

        // 4. Point Cloud 데이터를 계산하여 점 좌표를 얻어냄
        filtered_points = filtered_pc.calculate(f);

        // 5. 최종적으로 .ply 파일로 저장
        filtered_points.export_to_ply("test.ply", colored_depth);
        capture = false;
    }
}
```

**🔍 코드 설명**
1. **`if (capture)`**
    - 사용자가 **저장 버튼을 눌렀을 때만 실행**되도록 함.
    - 파일이 저장되면 `capture = false`로 변경하여 **한 번만 실행되도록 방지**.
2. **`filtered_data.poll_for_frame(&f)`**
    - 새로운 Depth 데이터 프레임을 **비동기적으로 가져옴**.
    - `poll_for_frame(...)` → 데이터가 준비될 때까지 기다리지 않고 **즉시 사용할 수 있는 데이터만 반환** (비동기 처리).
    - **⚡ 장점:** 카메라가 실시간으로 동작하는데, 불필요하게 대기 시간을 줄여줌.
3. **`colored_depth = color_map.process(f);`**
    - Depth 데이터에 색을 입혀 **시각적으로 확인할 수 있도록 변환**.
    - 예를 들어, **가까운 곳은 빨간색, 먼 곳은 파란색으로 변환**.
4. **`filtered_pc.map_to(colored_depth);`**
    - 포인트 클라우드 데이터에 색상 정보를 매핑.
    - 즉, **3D 좌표 데이터에 컬러 정보를 추가**하여, 이후 파일을 열었을 때 **색이 입혀진 상태**로 보이게 함.
5. **`filtered_points = filtered_pc.calculate(f);`**
    - Depth 데이터를 기반으로 **3D 좌표(Point Cloud)를 생성**.
    - 이 단계에서 **각 픽셀의 Depth 값을 3D 공간의 좌표(x, y, z)로 변환**.
6. **`filtered_points.export_to_ply("test.ply", colored_depth);`**
    - `.ply` 포맷으로 저장.
    - `.ply`는 **3D 스캔 데이터(Point Cloud)를 저장하는 포맷**이며, **MeshLab 등의 툴에서 열어볼 수 있음**.
    - `colored_depth`를 추가했기 때문에, **색 정보도 포함된 포인트 클라우드가 저장됨**.
7. **`capture = false;`**
    - 파일이 한 번만 저장되도록 **변수 초기화**.

---

**3️⃣ UI 체크박스로 필터 On/Off 추가 (ImGui 활용)**
```cpp
cpp
복사편집
bool filter_enabled = true;

if (ImGui::Checkbox("Enable Filter", &filter_enabled))
{
    if (filter_enabled) {
        // 필터 적용
        thr_filter.set_option(RS2_OPTION_MAX_DISTANCE, 0.300);
    } else {
        // 필터 해제
        thr_filter.set_option(RS2_OPTION_MAX_DISTANCE, 5.0); // 기본값으로 변경
    }
}

```

**🔍 코드 설명**
1. **`ImGui::Checkbox("Enable Filter", &filter_enabled)`**
    - ImGui(Immediate Mode GUI) 라이브러리를 사용하여 **체크박스를 생성**.
    - `filter_enabled` 변수의 상태를 기반으로 체크 여부를 결정함.
    - **체크하면 필터 적용, 해제하면 필터 비활성화**.
2. **`if (filter_enabled)` 조건문**
    - 필터가 활성화되면 `RS2_OPTION_MAX_DISTANCE` 값을 0.3m로 설정 (불필요한 데이터 제거).
    - 필터를 비활성화하면 최대 거리 제한을 5m로 변경하여 **전체 Depth 데이터를 볼 수 있도록 함**.

---
### **4. 주요 기능 리스트 (평가표에 적을 기능)**
**1. Depth 카메라 촬영**
(촬영 속도가 너무 느리면 실시간 스캔이 어렵고, 해상도가 낮으면 정밀한 측정이 불가능)

| 세부 기능 | 평가 기준 |
| --- | --- |
| 카메라 작동 여부 | 프로그램 실행 후 **3초 이내**에 정상 작동해야 함 |
| Depth + 컬러 이미지 캡처 | 촬영 버튼 클릭 후 **1초 이내**에 캡처 완료 |
| 촬영된 이미지 해상도 | 해상도 **1280x720 이상 유지** |

**2. Depth 이미지 및 컬러 이미지 분리**
(둘을 분리하면 **Depth 정보만 활용해 정밀한 3D 스캔 가능, 컬러 이미지가 필요하면 후처리로 추가 가능하기 때문에 따로 분리하는 게 중요**)

| 세부 기능 | 평가 기준 |
| --- | --- |
| Depth/컬러 이미지 분리 속도 | 이미지 캡처 후 **0.5초 이내**에 분리 완료 |
| 분리된 이미지 크기 | 원본 이미지 대비 **5% 이내 오차 허용** |
| 이미지 저장 용량 | **2MB 이하로 저장** (압축률 적용) |

**3. Depth 색상 표현**
(인간의 눈으로 **Depth 데이터를 직관적으로 이해할 수 있도록 시각화, 잘못된 색상 표현이 되면 Depth 값이 잘못 측정된 걸 빠르게 찾을 수 있음**)

| 세부 기능 | 평가 기준 |
| --- | --- |
| Depth 값을 색상으로 변환 | 변환 속도 **1초 이내** 유지 |
| 색상 표현 정확도 | 가까운 물체일수록 빨간색, 먼 물체일수록 파란색이 정확히 표현되어야 함 |
| 색상 계조 표현 | 동일 거리 영역의 색상 차이 **5% 이내** |

**4. 필터링 기능 (Max Distance = 0.3m)**
(발바닥만 스캔해야 하는데, 필요 없는 배경 정보까지 들어오면 안 됨, 불필요한 Depth 데이터를 제거하면 **데이터 크기를 줄이고 처리 속도를 향상**시킬 수 있음, 필터를 조정할 수 있어야 **다양한 조건(예: 다른 발 크기, 다른 거리)에서도 최적화** 가능)

| 세부 기능 | 평가 기준 |
| --- | --- |
| 0.3m 이상 Depth 데이터 제거 | 0.3m 이상 데이터 픽셀 값이 0으로 처리되어야 함 |
| 필터링 처리 속도 | 필터 적용 후 **0.5초 이내**에 반영 완료 |
| 필터 적용 후 Depth 데이터 크기 변화 | 원본 대비 데이터 크기 **20% 이상 감소** |

**5. Viewport 출력 (실시간 시각화)**
(**현재 스캔된 Depth 데이터가 잘 보이는지 즉시 확인 가능**해야 함, 실시간 Viewport가 없다면 매번 저장 후 열어봐야 하므로 **작업 과정이 비효율적,** FPS(초당 프레임 수)가 너무 낮으면 **화면이 끊겨서 조작이 어려워짐**)

| 세부 기능 | 평가 기준 |
| --- | --- |
| 렌더링 속도 | Viewport에서 FPS **30 이상 유지** |
| 색상 Depth 데이터의 렌더링 지연 시간 | **100ms 이하** |
| Viewport 내 해상도 유지 | **원본 대비 90% 이상 화** |

**6. Point Cloud 변환**
(Depth 데이터만으로는 **3D 형태를 직접 다루기 어려움, Point Cloud**로 변환하면 **3D 모델링 작업 및 후처리 가능,** 변환 속도가 느리면 실시간성이 떨어지고, 점 개수가 적으면 **디테일이 부족한 거친 모델이** )

| 세부 기능 | 평가 기준 |
| --- | --- |
| 변환 속도 | **2초 이내**에 변환 완료 |
| 생성된 Point Cloud의 점 개수 | **50,000개 이상 유지** |
| Point Cloud 밀도 균일성 | 특정 영역(중앙 기준 10cm²) 내 점 개수 차이 **10% 이하** |

**7. .ply 파일 저장 기능**
(Point Cloud 데이터는 **후처리(예: 메쉬 생성, 모델링, 분석)에 필수적, MeshLab 같은 소프트웨어에서 열어볼 수 있도록 파일 저장**해야 함)

| 세부 기능 | 평가 기준 |
| --- | --- |
| 저장 속도 | 저장 버튼 클릭 후 **1초 이내** 저장 완료 |
| 파일 크기 | 3D 데이터 저장 후 용량 **10MB 이하** |
| 데이터 손실률 | 원본 Point Cloud 대비 **99% 이상 데이터 유지** |

**8. 필터 On/Off 기능 (UI 체크박스 조작 가능)**
(필터를 적용하면 **불필요한 부분이 제거**되지만, 상황에 따라 필터가 필요하지 않을 수도 있음)

| 세부 기능 | 평가 기준 |
| --- | --- |
| UI 체크박스 클릭 반응 속도 | **100ms 이내**에 필터 적용 여부 반영 |
| 필터 On/Off 후 데이터 차이 | 필터 On 상태일 때 데이터 크기 **20% 이상 감소**, Off 상태일 때 원본 복원 |

**9. MeshLab을 활용한 검증 (저장된 .ply 파일 확인)**
(실제로 3D 모델이 제대로 만들어졌는지 확인할 방법이 필요, Point Cloud만 보고는 완성된 모델 품질을 알기 어려우므로 **MeshLab 같은 도구에서 확인하는 과정 필수,** 데이터가 정상적으로 저장되지 않으면 **후처리(예: 3D 프린팅, 분석)가 불가능**)

| 세부 기능 | 평가 기준 |
| --- | --- |
| MeshLab에서 로드 가능 여부 | 저장된 파일이 **5초 이내** 정상 로드 |
| 3D 모델 형태 유지 | 모델의 가장자리 오차 범위 **5% 이내** |
| 파일 내 점 손실률 | 저장된 .ply 파일의 점 개수가 원본 대비 **95% 이상 유지** |

---
### **5. 사용 센서 및 카메라**
- **Intel RealSense Depth Camera**: Depth 및 RGB 이미지 촬영
- **IMU 센서 (Optional)**: 카메라의 각도 보정 및 동작 인식에 사용 가능

---

### **6. 측정 방식 및 과정**
1. **초기 설정**
    - RealSense Camera를 특정 위치에 고정
    - 촬영 환경(조명, 배경) 세팅
2. **데이터 수집**
    - Depth 및 RGB 이미지 획득
    - Depth 데이터를 필터링하여 불필요한 정보 제거 (Max Distance 적용)
3. **데이터 처리**
    - Depth 데이터를 컬러로 변환하여 가시화
    - Point Cloud로 변환 후 `.ply` 파일 저장
4. **결과 검증**
    - MeshLab을 사용해 `.ply` 파일 검증 및 시각화

---
### **7. 참고할 유사 프로젝트**
1. **Intel RealSense Sample Projects**
    - https://github.com/intel/depthcamera-3d-model-web-demo
    - Intel에서 제공하는 Depth Camera 기반 3D 모델링 예제
2. **Open3D 활용 프로젝트**
    - http://www.open3d.org/docs/release/compilation.html#compilation-unix-python
    - Open3D를 활용한 3D 데이터 처리 및 시각화
3. **MeshLab 기반 3D 스캔 프로젝트**
    - https://verilog-119b.tistory.com/17?category=957514
    - MeshLab을 이용한 Point Cloud 후처리 및 메쉬 변환
4. **LIPS Scan 3D**
    - https://www.lips-hci.com/lipscan-3d-dekstop-free-trial
    - 3D 스캔 및 Depth 데이터 분석 툴

---
### **8. 추가 용어 설명**
- **Depth Camera (깊이 카메라)**: 물체까지의 거리(깊이)를 측정하는 카메라. RGB 카메라와 함께 사용하여 3D 데이터를 생성할 수 있음
- **Point Cloud (점군 데이터)**: 3D 공간에서 점들로 구성된 데이터 구조
- **Intel RealSense: Intel에서 개발한 Depth Camera 시리즈 및 관련 SDK. 3D 스캔, 얼굴 인식, 로봇 비전 등에 활용**
- **SDK (Software Development Kit, 소프트웨어 개발 키트): 특정 하드웨어나 소프트웨어를 개발할 때 필요한 라이브러리, API, 도구를 포함한 패키지**
- **RealSense SDK**: Intel RealSense 카메라를 제어하고 데이터를 처리하는 개발 도구
- **Max Distance (최대 거리 필터)**: 특정 거리 이상의 Depth 데이터를 제거하는 필터링 방식
- **.ply 파일 (Polygon File Format)**: 3D 점군(Point Cloud) 데이터를 저장하는 파일 형식. MeshLab과 같은 소프트웨어에서 시각화 가능
- imgui (Immediate Mode GUI): C++ 기반의 경량 GUI 라이브러리로, 실시간으로 UI를 렌더링하고 조작할 수 있음

---

## <박보성>
computer vision
3D position estimation
삼각측량
calibration
epipolar geometry

## **Computer Vision을 활용한 3D 위치 추정 (3D Position Estimation)**
3D 위치 추정(3D Position Estimation)은 **카메라 이미지 또는 영상에서 객체의 3D 좌표를 추정하는 기술**입니다. 

---
## **3D 위치 추정 기법**
### **① 단안 카메라 기반 (Monocular Vision)**
👉 하나의 카메라로 3D 위치를 예측하는 방식
- 카메라의 **내부 파라미터(intrinsic parameters)** 및 **왜곡(distortion)**을 보정
- **딥러닝**을 이용한 깊이 추정 (Depth Estimation) 활용
- **한계:** 단일 이미지에서는 깊이 정보를 직접 알 수 없기 때문에 정확도가 떨어짐

**📌 주요 기법**
- **SfM (Structure from Motion)**: 여러 장의 이미지에서 3D 구조를 복원
- **Deep Learning 기반**: CNN, Transformer 등을 이용한 깊이 예측 네트워크

---
### **② 스테레오 비전 (Stereo Vision)**
👉 두 개 이상의 카메라를 사용하여 객체의 깊이 정보를 추출
- 사람의 **양안 시차(Binocular Disparity)** 원리를 이용
- *삼각측량(Triangulation)**을 통해 3D 좌표를 계산

**📌 주요 기법**
- **Stereo Matching**: 두 개의 이미지 간 특징점을 대응시켜 깊이 맵(Depth Map) 생성
- **SLAM (Simultaneous Localization and Mapping)**: 카메라의 위치와 3D 구조를 동시에 추정

---
### **③ LiDAR 및 RGB-D 센서 기반**
👉 RGB 카메라와 함께 **깊이(depth) 센서**를 활용
- **LiDAR (Light Detection and Ranging)**: 레이저를 발사하여 반사된 시간을 측정해 3D 위치 계산
- **RGB-D 카메라** (예: Kinect, RealSense): RGB 이미지와 깊이 정보를 함께 제공

**📌 주요 기법**
- **Point Cloud Processing**: 3D 포인트 클라우드를 생성하고 분석
- **ICP (Iterative Closest Point)**: 3D 점군을 정합(Alignment)하여 위치 추정

---
## **3D 위치 추정 과정**
### **① 카메라 캘리브레이션(Camera Calibration)**
- **내부 파라미터(Intrinsic Parameters)**: 초점 거리(focal length), 주점(principal point) 등 보정
- **외부 파라미터(Extrinsic Parameters)**: 카메라의 위치 및 회전(Rotation, Translation) 보정

### **② 특징점 추출 및 매칭 (Feature Detection & Matching)**
- SIFT, ORB, SuperPoint 등 알고리즘을 사용하여 이미지의 특징점을 추출하고 대응점을 찾음

### **③ 깊이 맵 생성 (Depth Estimation)**
- 스테레오 비전, SfM, RGB-D 센서를 이용하여 각 픽셀의 깊이 정보를 예측

### **④ 삼각측량(Triangulation)**
- 두 개 이상의 카메라 또는 프레임을 활용하여 3D 좌표를 복원

### **⑤ 3D 포인트 클라우드 생성 및 정렬**
- SLAM, SfM을 통해 3D 공간상의 점군(Point Cloud)을 생성 및 정렬

## 평가표
| 발 길이 측정 기능 |  |  |
| --- | --- | --- |
| 발볼 넓이 측정 기능 |  |  |
| 발등 높이 측정 기능 |  |  |
| 발 아치 유형 분류 기능 |  |  |
| 3D 매쉬 형태 기능 |  |  |
|  |  |  |
| 측정 데이터 기반으로 러닝화 추천 기능 |  |  |
| 음성 안내 기능 |  |  |

---
## <백승욱>
## 발분석 워크플로우
![Editor _ Mermaid Chart-2025-03-22-092342.png](attachment:d1bec4ad-8dbf-4673-86c1-6cf2b78000df:Editor___Mermaid_Chart-2025-03-22-092342.png)

## 기능
1. 발 길이, 폭 자동 측정 
2. 발 아치 높이 및 발 둘레 등 3D 치수 측정 
3. 발바닥 압력 분포 분석 
4. 발 형상의 3D 모델 생성 
5. 결과 시각화 및 피드백 

## 필요 센서/카메라 종류
---
- 확인
    - RGB 카메라(라즈베리파이 카메라)
        - 일반 컬러 카메라로 발을 촬영하여 영상처리 알고리즘(OpenCV)으로 치수 산출
            - EX
                1. 위에서 발을 촬영한 이미지를 이진화하여 발 윤곽 검출 
                2. 픽셀 길이를 실측 단위로 환산하여 발 길이, 폭 계산 가능 
        - 라즈베리파이 카메라는 가격이 저렴, 라즈베리파이보드와 연동 쉬움
    - Depth 카메라
        - Intel RealSense → 한 번의 촬영으로 컬러 영상과 깊이 맵을 동시에 제공하여, 발의 3D 형상을 손쉽게 얻을 수 있음
        - 복잡한 스테레오 매칭이나 다중 사진 알고리즘 없이 실시간 3D 스캔이 가능하므로 구현 난이도 낮아짐
    - ToF 거리 센서
        - 적외선 빛을 발사한 후 반환 시간을 측정해 물체까지의 거리를 알려주는 초소형 모듈
        - 단일 ToF 센서로는 한 지점까지의 거리만 얻지만, 여러 개를 배열하면 발등 높이나 발 둘레를 여러방향에서 동시에 측정
            - 발을 둘러싼 링에 다수 설치 → 발 옆면 윤곽 다점 측정
            - 상단에 설치 → 발등 가장 높은 곳까지의 높이를 자동으로 잴 수 있음
    - LiDAR 스캐너
        - 레이저를 회전시키거나 스캔하여 거리 데이터를 얻는 센서로 2D 또는 3D 센서
            - 2D LiDAR
                
                → 이것을 수직으로 세워놓고, 회전시키면 발 주변의 옆면 윤곽을 얻을 수 있음 
                
                → 수평으로 놓고, 회전시키면 전체 발의 외곽 3D 데이터 획득 가능 
                
        - 주변 환경 광원의 영향을 받지 않고, 정밀한 거리 제공
        - 센서 자체 가격이 비싸고, 데이터 처리 알고리즘이 필요
        - 발의 외형 3D 포인트 클라우드 생성을 위한 옵션으로 고려
    - 적외선/초음파 센서
        - IR 거리 센서 / 초음파 센서
            - 측정 거리에서 물체까지의 거리를아날로그/디지털 신호로 알려줌
            - 아두이노와 연동이 쉽고, 단순 길이 측정에 활용 가능
            - EX
                
                → 신발 깔창 길이를 자동으로 재는 기기에서는 발가락이 닿는 위치를 IR포토인터럽터로 감지하고, 뒤꿈치 위치까지의 거리를 초음파 센서로 측정하는 식으로 구현 
                
        - 해상도, 정밀도가 다소 떨어짐
        - 대략적 치수 측정에 적합
    - 하중 센서
        - 압력이나 무게 중심을 재기 위함
        - 로드셀, FSR 센서
            - 로드셀 → 저울에 쓰이는 스트레인 게이지 기반 센서
                1. 4개의 로드셀을 판의 네 모서리에 달아 발이 가한 하중을 측정하면 앞꿈치-뒤꿈치/좌-우로 가해진 무게 분포를 산출 가능
            - FSR → 압력에 따라 저항이 변하는 박막 센서
                1. 여러 개를 깔창 형태로 배열하여 분포형 압력 매트 구성  

## Depth Camera 기반
---
- 확인
    - 준비물
    
    | 구성 | 용도 |
    | --- | --- |
    | Depth Camera (예: Intel RealSense D435) | 3D 포인트 클라우드 수집 |
    | 라즈베리파이 5  | 센서 데이터 처리 |
    | 투명 아크릴판 또는 바닥 플랫폼 | 사용자가 발을 올려놓을 구조물 |
    | ToF 거리 센서 | 발등 높이 측정 |
    |  FSR 압력 센서 매트 | 발바닥 압력 분포 분석 |
    | 소프트웨어   | 데이터 처리 및 시각화 |
    - Depth Camera 사용시 전체적 구상도
        
        ![Editor _ Mermaid Chart-2025-03-22-124143.png](attachment:2b92d214-7877-4ce0-891a-baa79a8fa8c3:Editor___Mermaid_Chart-2025-03-22-124143.png)
        
    - 전체 시스템 순서 및 작동 방식
        1. 사용자 발 
            1. 발의 움직임이 적어야 
        2. Depth 카메라 (RealSense D435) 
            1. 역할: 3D 측정의 핵심 센서, 거리 정보 
                1. x,y,z 좌표 획득 
                    - x,y는 이미지의 픽셀 위치 (가로, 세로)
                    - z는 해당 픽셀의 거리값 (Depth)
                        
                        → 이 3개를 합쳐서 (x,y,z) 포인트 클라우드 생성 
                        
        3. Raspberry Pi 5
            1. 역할: 모든 센서 제어, 데이터 수집, 분석 알고리즘 실행 
            2. 작동 방식 
                - Depth Camera, ToF, FSR 센서 연결
                - RealSense SDK, I2C, ADC 등으로 데이터 읽기
                - 분석 결과 출력 또는 저장
        4. 3D 포인트 클라우드 생성
            1. 역할: Depth Map → 실제 공간의 3D 점들로 변환
            2. 작동 방식
                - 각 픽셀(x,y)와 해당 z 값을 통해 공간 좌표 계산
                - 카메라 내부 파라미터(fx, fy등)을 이용해 실제 거리로 변환
            3. 출력 : 수천~수십만 개의 (x,y,z) 좌표 점군 
        5. 전처리 - 노이즈 제거 및 발 영역 분리
            1. 역할: 배경, 바닥 등 불필요한 점 제거하고 발 부분만 남김 
            2. 작동 방식 
                - 높이(z) 기준 필터링
                - DBSCAN, ROI 기반 분할 등으로 발만 추출
        6. FSR 압력 센서 
            1. 역할: 발바닥에 가해지는 압력 분포 측정 
            2. 작동 방식 
                - 발 아래 매트에 FSR 배열 설치
                - 각 센서가 압력을 아날로그 전압으로 출력
                - 아날로그 입력 → Raspberry Pi → 실시간 압력 데이터 수집
            3. 활용
                - 압력 히트맵 시각화
                - 발 뒤꿈치 / 앞꿈치 하중 비율 분석
                - 무게 중심 추정
        7. ToF 거리 센서 
            1. 역할:  발등 높이 측정 보조 
            2. 작동 방식 
                - 발 위쪽에서 수직으로 거리 측정
                - Depth 카메라의 보조 수단으로 사용
                - VL53L0X  → 최대 2m 까지, 센서의 정확도는 **기껏해야 ± 3 %에서 최적 조건에서 ± 10 % 이상**으로 지정)
            
            <aside>
            💡
            
            ✏전처리 이후 FSR과 ToF를 하는 이유 
            
            - FSR
                - 발바닥에 **어디에 얼마나 힘이 가해졌는지**를 보여줌
                - 이건 형상 정보랑은 **다른 차원의 데이터** (Z축으로 변형될 수도)
                - 그래서 **형상 분석한 다음**, “이 발에 체중이 어디 실렸는가”를 확인하는 용도
            - ToF
                - 발등처럼 **Depth 카메라가 잘 안 보는 곳**을 따로 보완함
                - 예를 들어 발등이 그림자지거나 반사가 많으면 정확한 z값을 못 얻을 수 있는데, 이때 ToF로 **보정된 거리값**을 추가
            </aside>
            
        
        1. 발 치수 분석 
            1. 역할: 3D 점군 기반으로 발 길이, 너비, 아치 높이 등 계산 
            2. 작동 방식 
                - x 축 최대/최소 → 발 길이
                - y 축 폭 계산 → 발볼 너비
                - z 축 높이 → 발등, 아치 높이
                - 단면 슬라이싱으로 아치 형태 파악
        2. 결과 시각화 
            1. 역할: 사용자가 측정 결과를 직관적으로 확인 
            2. 작동 방식
                - Open3D, Matplotlib 등으로 3D 형상 표시
                - 히트맵(압력), 수치(길이, 높이) 표시
                - 정면, 측면 회전 가능 UI 제공
        3. UI/UX
            1. 결과 표시 
            2. 터치스크린, 디스플레이 
        4. 결과 저장 및 전송 
            1. 사용자의 발 데이터를 로컬/서버 저장 
            2. 사용자가 원할시에 QR코드 방식과 같은 것 활용 
    

## Depth Camera 평가표
---
- 확인
    
    
    | **목표** | **세부 기능** | **세부 내용 / 구현 방식** |
    | --- | --- | --- |
    | 자동 발 측정 기능 | 발 길이 측정 | Depth 카메라로 얻은 포인트 클라우드에서 x축 최대–최소 거리 계산 |
    |  | 발볼 넓이 측정 | 일정 z 위치 단면에서 y축 최대–최소 폭 측정 |
    |  | 발등 높이 측정 | z축 최고점 - 기준 바닥면 (또는 ToF 센서로 보완) |
    |  | 발 아치 유형 분석 | 발 중간 단면 슬라이싱 → 아치 곡률 분석, 기준값에 따라 평발/보통/고아치 분류 |
    | 정밀한 발 측정 데이터 획득 | 포인트 클라우드 기반 | 수천~수십만 개의 (x, y, z) 점 추출 |
    |  | 거리 오차 최소화 | 카메라 보정 파라미터 적용, ±2mm 정밀도 유지 |
    |  3D 기반 결과 시각화 | 점군 시각화 | Open3D, PCL 등으로 실시간 회전 가능한 발 형상 출력 |
    |  | 메쉬 모델 생성 | 포인트 클라우드 → 메쉬 변환 (BPA, Poisson 등) |
    | 실시간 데이터 처리 및 분석 기능 | 프레임 단위 분석 | Depth 스트리밍 입력 처리 및 실시간 시각화 |
    |  | 자동 치수 계산 | 시점 고정 후 자동 거리 계산 루틴 실행 |
    | 비접촉 + 고정 측정 설계 | 카메라 고정 구조 | 위 또는 비스듬한 각도 고정 설치, 발판 고정 |
    |  | 사용자 위치 유도 | 투명 판 + 유도선 또는 실시간 카메라 미리보기 제공 |
    | 센서 융합 | ToF 거리 보완 | Depth 카메라 사각지대 보완용 (발등 중심 수직 거리 측정) |
    |  | FSR 압력 분석 (선택) | 압력 히트맵 추가 분석, 무게 중심 계산 |
    | 사용자 피드백 & 자동화 | UI 및 안내 | 측정 완료 후 치수 안내, 시각적 결과 + 음성 안내 (TTS) |
    | 인체공학적 기기 설계 | 안정적 발판 구조 | 미끄럼 방지 처리, 기준 위치 안내 포함 |
    |  | 휴대 및 설치 용이성 | Depth 카메라 1대 기반, 이동 가능한 구조 |
    
    ### 평가 항목 예시
    
    | 항목 | 평가 기준 |
    | --- | --- |
    | 발 길이 정확도 | 오차 ±2mm 이내 |
    | 발 아치 판단 | 3단계 분류 정확도 ≥ 90% |
    | 시각화 인터페이스 | 3D 회전, 수치 표시 여부 |
    | 실시간 처리 | 측정, 분석 결과 출력 ≤ 5초 |
    | 사용자 편의성 | 안내, 조작, 설계 구조 평가 |
    | 비용 효율성 | 150만원 이하 구성 유지 |

## 일반 RGB 카메라 + 다각도 촬영 → 3D 복원
---
- 확인
    
    ![Untitled diagram-2025-03-23-042418.png](attachment:1166f086-baaa-4891-8d50-0919593d0f65:Untitled_diagram-2025-03-23-042418.png)
    

## 일반 RGB 카메라 평가표
---
- 확인
    
    
    | **목표** | **세부 기능** | **세부 내용 / 구현 방식** |
    | --- | --- | --- |
    | 자동 발 측정 기능 | 발 길이 측정 | 3D 포인트 클라우드에서 x축 최대–최소 거리 계산 |
    |  | 발볼 넓이 측정 | y축 기준 단면에서 양 끝 지점 간 거리 계산 |
    |  | 발등 높이 측정 | z축 최고점 계산 or ToF 센서 보완 사용 |
    |  | 발 아치 유형 분석 | 발 중간 단면의 높이/곡률 분석 → 아치 유형 분류 (평발/보통/높은 아치) |
    | 정밀한 발 측정 데이터 획득 | 고해상도 3D 모델 생성 | RGB 사진 다각도 촬영 → Photogrammetry(사진측량) 활용 |
    |  | 발 치수 자동 계산 | Open3D 또는 Python 기반 거리 계산 알고리즘 |
    | 3D 기반 결과 시각화 | 포인트 클라우드 시각화 | Open3D, Matplotlib 등으로 3D 점군 시각화 |
    |  | 메쉬 모델 생성 | Meshroom 등으로 메쉬 생성 → STL/OBJ 저장 |
    | 실시간 데이터 처리 및 분석 기능 | 압력 데이터 수집 | FSR 센서로 압력값 수집, 아날로그 → 디지털 변환 |
    |  | 실시간 분석 | 발바닥 압력 히트맵 시각화, 무게중심 이동 표시 |
    | 비접촉 + 최소 움직임 측정 설계 | 고정형 촬영 구조 | 발판 위에 사용자가 정지한 상태로 서 있는 구조 설계 |
    |  | 단일 촬영 → 자동 처리 | 사진 수집 후 자동 분석 파이프라인 구성 |
    | 사용자 피드백 반영 | 환경 보정 기능 | 조도 감지 → 조명 안내 / 발 위치 벗어남 감지 |
    |  | 발 크기에 따른 카메라 위치 조정 (옵션) | 측정 전 확인 이미지 표시로 위치 안내 |
    | 자동 안내 시스템 | 음성 안내 기능 | 측정 시작, 자세 유지, 결과 안내 음성 출력 (TTS 활용) |
    | 인체공학적 기기 설계 | 발판 구조 설계 | 투명 아크릴 or 안정적 소재 사용, 마커 포함 |
    |  | 사용 편의성 고려 | 발 위치 유도선, 손쉬운 진입/퇴장 동선 확보 |
    
    ### 
    
    | 분류 | 항목 | 설명 |
    | --- | --- | --- |
    | 정확도 | 길이/폭 오차율 | 실측 대비 ± 몇 mm |
    | 편의성 | 사용자 만족도 조사 | 측정 시간, 이해도, 조작성 등 |
    | 확장성 | 신발 추천 알고리즘 적용 가능성 | 룰 기반 or 머신러닝 기반 추천 연동 |
    | 경제성 | 부품 단가 총합 | Depth 카메라 X → 저비용 유지 여부 |

## 비고
---
- Depth 카메라를 활용한 발 분석기 관련 논문 (우리가 하려는 것과 똑같다고 봐도 무방)
    
    [sensors-17-01796-v2.pdf](attachment:c3470b9c-5bb6-4110-ab13-d960e0c83dfb:sensors-17-01796-v2.pdf)
    
    - gpt로 정리한 것
        - 확인
            
            이 논문은 **RGB-D 카메라를 활용해서 자동으로 발 아치(arch) 관련 지표를 측정하는 시스템**을 소개
            
            ---
            
            ### **논문 제목**
            
            **A Foot-Arch Parameter Measurement System Using a RGB-D Camera**
            
            ---
            
            ### **1. 연구 목적**
            
            기존의 발 아치 측정 방식은 **측정자의 숙련도에 따라 오차가 크고 반복 측정이 어려움**.
            
            이를 해결하기 위해 **RGB-D 카메라(색상+깊이 정보)**를 활용해서 **자동으로 발 아치 지표**를 측정하는 시스템을 개발
            
            ---
            
            ### **2. 측정하는 발 아치 지표 세 가지**
            
            1. **Arch Index (AI)**: 발의 중간 부분 면적 / 전체 발바닥 면적 (발가락 제외)
            2. **Arch Width (AW)**: 발 안쪽 아치의 너비
            3. **Arch Height (AH)**: 발 안쪽 아치의 높이
            
            ---
            
            ### **3. 시스템 구성**
            
            - **하드웨어**:
                - 투명 아크릴 판 위에 발을 올리고,
                - 그 아래에 **Intel RealSense RGB-D 카메라**를 설치해 발바닥을 촬영함.
            - **소프트웨어 처리 흐름**:
                1. **전처리**:
                    - 깊이 이미지의 노이즈 제거 (중간값 필터 등)
                    - 색상/깊이 데이터 정렬 및 통합 (6D 데이터 생성: x, y, z, r, g, b)
                2. **접촉 영역 추출**:
                    - 발이 바닥에 닿은 부분을 인식 (MRF + 클러스터링 기법 활용)
                    - 평면 추정으로 바닥 위치를 알아내고, 거리를 기준으로 접촉 여부 판단
                3. **지표 계산**:
                    - **AI**: 발바닥을 3등분해서 중간 면적 비율로 계산
                    - **AW & AH**: 발의 안쪽 선(MBL)을 기준으로 높이와 너비를 측정
            
            ---
            
            ### **4. 실험 및 결과**
            
            - **11명의 피험자**, 3일간 99개의 데이터 수집
            - 실제 측정값과 비교했을 때 **평균 오차는 매우 작음**:
                - AI: -0.17%
                - AW: 0.95mm
                - AH: 0.52mm
            - **측정 시간**: AI, AW, AH 모두 **10초 이내**에 자동 측정됨
            
            ---
            
            ### **5. 장점**
            
            - **비접촉식, 자동화, 빠름**
            - 기존 잉크를 묻히고 손으로 자/눈금자로 측정하던 방식보다 훨씬 **정확하고 반복성 우수**
            - 향후 **동적 보행 분석**으로도 확장 가능
            
            ---
            
            궁금한 부분 더 자세히 알려줘도 되고, 이걸 기반으로 발표 자료나 보고서 형태로 정리해달라고 해도 가능
            
            **Intel RealSense**는 **Depth(깊이) 카메라**
            
            정확히는 **RGB-D 카메라**라고 부르고, 여기서 **"RGB-D"는 RGB + Depth**의 줄임말
            
            ---
            
            ### **Intel RealSense = RGB + Depth 센서**
            
            - **RGB 카메라**: 일반적인 컬러 이미지 촬영 (색상 정보)
            - **Depth 센서**: 각 픽셀이 **카메라에서부터 얼마나 떨어져 있는지(깊이)** 측정
            
            ---
            
            ### **논문에서 쓴 모델**
            
            - **Intel RealSense F200** 모델 사용
            - 이 모델은 **Time-of-Flight 방식**이 아니라 **적외선(IR) 구조광 방식**을 씀
            (패턴을 쏴서 거리를 계산하는 방식)
            
            ---
            
            ### **왜 이걸 썼냐면?**
            
            1. **발바닥의 3D 형상**을 정확하게 측정하려면 깊이 정보가 필요하고,
            2. 동시에 **피부의 색 변화**나 발 모양을 시각적으로 분석하려면 RGB도 필요해서
            → **RGB-D 카메라**가 딱 적절함.
            
- 상기와 같음
    
    [3D foot scanning using multiple RealSense cameras.pdf](attachment:a29ffedf-9014-416a-8a29-378f67ecbf3f:3D_foot_scanning_using_multiple_RealSense_cameras.pdf)
    
    - gpt로 정리한 것
        - 확인
            
            이 논문 **「3D foot scanning using multiple RealSense cameras」**는 Intel RealSense RGB-D 카메라를 사용하여 **저비용, 고정밀의 양발 동시 3D 스캐닝 시스템**을 설계한 연구
            
            ---
            
            ### ✅ 논문의 핵심 목적
            
            - 신발 맞춤 제작, 지능형 사이즈 추천, 족부 질환 진단을 위한 **정확한 3D 발 모형 획득**이 목표.
            - 이를 위해 **4개의 Intel RealSense SR300 카메라**와 **하나의 PC**를 이용한 시스템을 설계.
            
            ---
            
            ### 📌 시스템 구성
            
            1. **하드웨어**
                - **Intel RealSense SR300 (x4)**: 깊이 + 컬러 영상 동시 출력 가능.
                - **카메라 배치**: 정사각형 플랫폼의 네 모서리에 카메라 배치, IR 센서 기준으로 중앙을 향해 대각선 방향으로 설치.
                - **스캔 영역**: 36cm 정사각형 내에 양발을 올려놓고 스캔.
            2. **소프트웨어**
                - 자동 보정, 스캔, 정합, 재구성, 측정 등 전체 프로세스 구현.
            
            ---
            
            ### 🧠 주요 기술 기법
            
            ### 1. **자동 보정 (Calibration)**
            
            - *타워형 블록 (Tower-type block)**을 이용해 여러 대의 카메라를 자동으로 정렬 및 보정.
            - 블록에 있는 **원형 마커**를 감지 → 각 카메라의 좌표계를 월드 좌표계로 정렬.
            - **단위 사원수(quaternion)** 방식으로 회전과 평행이동 변환 매트릭스 계산.
            
            ### 2. **스캐닝 프로세스**
            
            1. **부분 뷰 수집**: IR 간섭을 방지하기 위해 **카메라 순차 작동**.
            2. **평균화된 깊이값**을 사용해 안정적인 포인트 클라우드 생성.
            3. **노이즈 제거**
                - **밴드패스 필터**: 거리 범위로 잡음 제거.
                - **반사 노이즈 제거**: IR 이미지를 이진화해 **접촉면의 반사 노이즈 제거**.
            4. **정합(Registration)**:
                - **초기 정렬** + **ICP 알고리즘**으로 정밀 정합.
            5. **비가시 영역 보정**:
                - **위쪽**: 수평 절단면 추정 후 채움.
                - **바닥면**: 포물선 형태로 채움.
            6. **재구성**:
                - **Poisson surface reconstruction** 기법으로 매쉬 모델 생성.
            
            ---
            
            ### 👣 발 치수 측정
            
            - 스캔된 양발 포인트 클라우드를 좌/우 분리.
            - 특징점 추출(P1~P8) 후 다음 치수 계산:
                - 발 길이(L), 발 너비(W), 볼 둘레(C1), 발등 둘레(C2), 단축 뒤꿈치 둘레(C3)
                - 중족부 높이(H1), 족근부 높이(H2), 거골부 높이(H3)
            
            ---
            
            ### 🧪 실험 결과
            
            | 평가 항목 | 결과 |
            | --- | --- |
            | **정확도** | 평균 오차 약 **0.95mm** 수준 (보정 정확도) |
            | **스캔 시간** | 전체 약 **14초**, 사용자는 **4초만 서 있으면 됨** |
            | **자동 측정 정확도** | 수동 측정과 **유사한 수준**, 편차 ±0.6mm 내외 |
            | **적용 가능성** | 남녀노소 20명 대상 실험에서 모두 성공 |
            
            ---
            
            ### 📍 장점
            
            - **저비용 구성** (RGB-D 카메라만 사용)
            - **자동화된 보정 시스템**
            - **빠르고 정확한 3D 발 측정**
            - **양발 동시 측정 가능**
            
            ---
            
            ### ⚠️ 한계 및 향후 과제
            
            1. **카메라 간 간섭 방지 위해 순차 작동** → 속도 느려짐
            2. **사람의 미세 움직임**으로 정합 오차 발생
            3. **비강체 움직임 고려 부족** → 향후 **시간에 따른 형태 변화** 분석, **실시간 재구성**, **보행 분석** 연구 예정
            
            ---
            
- Depth 카메라를 활용해 사람의 발 표면을 3D로 추적
    
    [ultiple Depth Sensor Setup and Synchronization for Marker-Less 3D Human Foot Tracking in a Hallway” (Gutta et al., 2019).pdf](attachment:352c5cda-d828-44bb-b709-9107c2fb17b8:ultiple_Depth_Sensor_Setup_and_Synchronization_for_Marker-Less_3D_Human_Foot_Tracking_in_a_Hallway_(Gutta_et_al._2019).pdf)
    
    - gpt로 정리한 것
        - 확인
            
            이 논문은 병원 복도 같은 좁은 공간에서 **사람의 발 움직임을 마커 없이 (marker-less)** 추적하기 위해 **여러 대의 Intel RealSense D415 깊이 센서(depth sensor)**를 이용한 실험과 방법론을 다룬 연구
            
            ---
            
            ## 🔍 연구 목적
            
            - **목표:** 복도에서 사람의 **발 표면을 3D로 추적**할 수 있는 최적의 센서 배치를 찾고, **시간 및 공간 동기화(synchronization)**를 통해 정확한 포인트 클라우드(point cloud)를 생성하는 것.
            - **이유:** 기존의 마커 기반 시스템은 걸음걸이에 영향을 줄 수 있고, 하나의 센서로는 좁은 복도 공간에서 완전한 데이터를 얻기 어렵기 때문.
            
            ---
            
            ## 🧪 연구 방법 요약
            
            ### 1. **시뮬레이션 설계**
            
            - **Blender 소프트웨어**를 사용해 실제 크기의 복도, 사람, 센서를 모델링하고 애니메이션으로 걷게 만듦.
            - 3가지 센서 배치 방식 실험:
                1. **2개 센서:** 복도 양쪽 모서리에 배치
                2. **4개 센서:** 모든 코너에 1대씩
                3. **6개 센서:** 더 짧은 거리의 고밀도 센서 배치 (정확도 향상 목적)
            - 목표: 센서 시야에 보이는 **발 표면의 비율(visibility percentage)** 측정
            
            ### 결과:
            
            - 4개 센서 → **97.07% 가시성**
            - 6개 센서 → **96.18% 가시성 + 더 넓은 추적 거리(7.15m)**
            - 2개 센서 → 가시성 낮음 (**83.33%**)
            
            ---
            
            ### 2. **실험실에서의 물리적 테스트**
            
            - 실제 **Intel RealSense D415 센서 6대**를 동일한 방식으로 설치
            - 센서 높이: 바닥에서 **0.8m**
            - 각 센서간 거리: 가로 **1.4m**, 세로 **1.8m**
            
            ---
            
            ### 3. **시간 동기화 (Time Synchronization)**
            
            - 각 센서가 **서버 역할의 노트북**에 연결, 이 노트북들은 **클라이언트 PC**에 유선(Ethernet)으로 연결됨.
            - 클라이언트가 동기화 명령을 보내 **모든 센서가 30ms 이내**에 동시에 데이터를 캡처하게 함.
            - 프레임 해상도: 848 × 480, 60 fps
            - 프레임당 RGB + Depth 이미지 저장
            
            ---
            
            ### 4. **공간 동기화 (Spatial Synchronization)**
            
            - 센서 간 위치를 맞추기 위해 **체스보드 캘리브레이션(OpenCV)** 사용
            - 센서의 내부/외부 파라미터를 이용해 **모든 센서 데이터를 하나의 기준 좌표계로 변환**
            
            ---
            
            ### 5. **포인트 클라우드 생성 및 병합**
            
            - 각각의 센서에서 얻은 3D 포인트를 변환 행렬을 통해 **하나의 좌표계로 병합**
            - **노이즈 제거 및 스무딩** 처리
            - 정확도 테스트: 반지름 **119.6mm인 농구공**을 두고 **RANSAC**으로 구 형태 모델링
                - 평균 오차(RMS): **11.83mm**
                - 실측 반지름과 비교시 편차 **±10mm 내외**로 정확도 확보
            
            ---
            
            ## ✅ 결론
            
            - 최소 **4개의 센서를 대각선 배치**하면 사람의 발 추적이 거의 완벽하게 가능함
            - RealSense D415는 여러 대를 동시에 써도 간섭 없이 잘 작동
            - **가까운 거리에서 사용할수록 정확도가 높아짐**
            - 시간 및 공간 동기화 기법 모두 안정적으로 작동함
            
            ---
            
            ## 📌 논문의 의의
            
            이 논문은 특히 **병원이나 재활시설의 복도**처럼 실사용 환경을 고려한 연구로서, **실제 적용 가능성**이 높은 점이 장점이야. 그리고 비교적 저렴한 RealSense 센서를 잘 활용했다는 점에서도 실용적인 연구라고 볼 수 있음
            
            ---
